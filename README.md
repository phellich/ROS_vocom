# Xplore - Human-Rover Communication

Welcome to the Human-Rover Communication project GitHub. The goal is to develop a rover capable of recognizing, understanding, and executing vocal commands (e.g., _Hello Kerby, go forward for 3 meters and turn 70 degrees left. Goodbye!_). Navigation and other commands must be highly precise, leaving no room for interpretation by the model (angle or distance). The system also considers the sentiment in your voice: e.g. if stress or fear is detected, the rover will execute actions faster. 

This GitHub repository serves to enhance reproducibility, documentation, and installation guidance.

## Table of Contents

- [Introduction](#introduction)
- [Model Overview](#model-overview)
  - [Speech-to-Text](#speech-to-text)
  - [Text-to-Commands](#text-to-commands)
  - [Sentiment Analysis](#sentiment-analysis)
  - [Command Execution](#command-execution)
- [Accepted Commands](#accepted-commands)
- [Installation](#installation)
- [TODO Ideas](#todo-ideas)

## Introduction

The workflow of the system is as follows:

![Project Overview](docs/overview.png)

The microphone used is a [Multidirectional Microphone](https://www.logitech.com/fr-ch/shop/p/yeti-orb-gaming-microphone.988-000551) from Logitech, which will be set on the rover. The model is designed to be lightweight, work offline, and recognize English vocal commands.

A switch on the Control Station (CS) will control whether the vocal command recognition system is activated or not.

When the switch is activated, a ROS node called `vocom_node` starts running. It subscribes to the `CS/vocom` topic (receiving a boolean value from the switch) and publishes [Joy messages](https://docs.ros2.org/foxy/api/sensor_msgs/msg/Joy.html) to another topic. The idea is to create fake joystick commands for manual navigation mode of the rover, which are then processed as if the joystick or gamepad buttons were actually used. This `Joy` message is further processed into a [Twist message](https://docs.ros2.org/foxy/api/geometry_msgs/msg/Twist.html) for execution (see Xplore doc).

At the same time when the switch is activated, the node starts a PyAudio stream from the microphone. First, it invokes the Speech-to-Text (S2T) model, followed by the Text-to-Commands (T2C) model. Finally, it checks the creation or modification of `commands.json` file written by T2C and sequentially executes the series of commands it contains. Throughout this workflow, several print statements ensure transparency, allowing the user to verify the rover's understanding from the CS. These include detecting wake words, the Whisper model's speech recognition result, the JSON generated by the LLM, and the executed commands.

## Model Overview

### 1. Speech-to-Text

As shown in the Project Overview, the S2T process combines two models:

First, Vosk is used. It has a lower WER rate but is less robust to noise. However, it is ideal for background processes for two reasons:  
1. It is lightweight (~50MB in size / ~300MB of RAM during runtime for the Small English Vosk 40M model).  
2. It only needs to recognize specific distinguishable words:  
   - `Hello` is the wake word. It signals the user is about to give commands, and the system starts recording audio.
   - `Goodbye` is the sleep word. It signals the end of user instructions, stopping the audio recording (which is then passed to Whisper).  
   
[comment]: <> (- `Stop` is the stop word. If spoken during command execution, it halts all actions.)

Second, the state-of-the-art Whisper model from OpenAI is used. Whisper Base (74M) is robust against Martian-like or crowd noise. It achieves better performance with 30-second-long audio clips due to its attention mechanism. This is why Whisper is not used for live recognition of wake words (it also requires more computational resources). Whisper requires about 1GB of VRAM during runtime but only runs for a few seconds after the audio is recorded.  

Below is an example demonstrating the improvement between Vosk and Whisper for speech recognition (in recognized text, punctuation, and capitalization):

<img src="docs/S2T_demo.png" alt="Speech Recognition Example" width="500">

### 2. Text-to-Commands

This component converts the unstructured, transcribed vocal instructions into a structured JSON format. The JSON contains a series of commands, each with specific properties and details. The schema is as follows:

```json
{
  "execution_speed": "string (either fast, slow, or default)",
  "commands": [
    {
      "command": "string (either 'displacement', 'rotation', 'drill', or 'not_a_command'). Depending on the value of 'command', other fields are populated as follows:
   - 'displacement': Requires 'direction' and 'distance'.
   - 'rotation': Requires 'direction' (as '180_turn' or '360_turn') and 'angle'.
   - 'drill': Requires 'drill_additional_info'.
   - 'not_a_command': All other fields must be null.",
      "direction": "string (either 'forward', 'backward', 'right', 'left', '180_turn', '360_turn' or null, depending on the command)",
      "distance": "positive integer (in meters, null otherwise)",
      "angle": "positive integer (in degrees, null otherwise)",
      "drill_additional_info": "string (optional, details on drill instruction, or null if not applicable)"
    }
  ]
}
``` 

This schema was chosen for two reasons:
1. It is easier for the LLM to generate a valid instance (flat easier than nested).
2. It simplifies the addition of new commands.

The model used is Llama 3.2 3B, prompted with the following context:

```
You are assisting a rover's navigation and control system to interpret user instructions accurately. Your task is to identify specific commands (such as "move," "turn", or "not a command") and their details (such as distance or angle) from the provided instructions. 

The output should be a valid JSON instance that matches the schema below. Do not include any explanations, preambles, code snippets, or additional input-output examples. Only provide one JSON output in response to the last input. If no valid command is found, generate a single "commands" instance where all fields are null.
```

This system context is also supported by a few-shot learning approach, which has shown to improve performance. Below is an example of a complete prompt:

<img src="docs/T2C_demo.png" alt="Prompt ex" width="400">

The model is called via [Llama.cpp](https://github.com/ggerganov/llama.cpp) framework. It has a [`JSON Schema Mode`](https://llama-cpp-python.readthedocs.io/en/latest/#json-mode) that guarantees that the output will be a valid JSON instance follwing the schema specified. It has also the advantage to choose the model. I am using quantitized LLama-3.2-3B-Instruct-Q6_K_L.gguf version find on [Hugging Face](https://github.com/ggerganov/llama.cpp). 

### 3. Sentiment analysis
TO DO 

### 4. Commands execution
Once the commands are loaded into a JSON file, they are iterated through and executed sequentially. Each command is translated into and published as a Joy message. 

See the next section for details about command interpretation for execution:
- Linear displacement: default speed is $ 0.5 m/s $ ($= 0.5$ publicated in Joy)
- Rotation: default speed default speed is $ 0.5 rad/s $ ($= 0.5$ publicated in Joy). 

For both navigation commands, the publication duration is calculated using the formula $ t = \frac{d}{v} $.

At any time, you can deactivate the Vocal command system and it will stop (including during a continous publication of Joy message, so it is safe is there is a critc situation). 


## Accepted commands
The currently accepted commands are:

- `move`: Requires the `direction` field (either `forward` or `backward`) and the `distance` field (in meters, positive integer). If no `distance` is specified, the rover will not move. If a `distance` is specified but no `direction`, the rover defaults to moving forward.

- `turn`: Requires the `direction` field (either `left`, `right`, `180_turn`, or `360_turn`) and the `angle` field (in degrees, positive integer). If `direction` is `left` or `right` with angle as null or 0, it defaults to turning 90 degrees. If the `direction` is `180_turn` or `360_turn` but no `left` or `right` is specified, it defaults to turning left.

E.g., if you want to go diagonnally, first indicate to turn an angle and then to advance in that direction. 
Drill + manual nav ou auto mode + mode de deplacement normal ou latéral. Dire qu'on fait seq et que les rotations sont en mode de deplacement crab. 


## Installation

Docker must be installed and running on your system. For guidance, refer to this [Xplore repository](https://github.com/EPFLXplore/ROS_Software_Assignement/blob/master/docs/InstallLinux.md). For additional guidance, you can learn to create a ROS package and run nodes by following the instructions provided in this [GitHub repository](https://github.com/EPFLXplore/ROS_Software_Assignement/blob/master/docs/Level2.md).

### Build and Run the Docker Container
1. Navigate to the folder `docker_humble_desktop`. If the Docker container is not already running, start it using the run.sh script for Linux. Then access the docker container terminal.
```sh
./run.sh
docker exec -it epflxplore_vocal_command bash
```
If an error occurs showing the missing command `docker-compose`, install it using `sudo apt-get update && sudo apt-get install -y docker-compose`

### Build and Run the Ros Package
2. In the dev_ws directory, build the package:
```sh
cd ~/dev_ws/
colcon build
```
This command compiles the package and sets up the necessary environment.

3. After building, source the setup file to overlay the workspace on your environment:
```sh
. install/setup.bash
```
This command sets up the environment variables needed to run the nodes. Now, your terminal is aware of the existing nodes. You will have to execute this command on every terminal you open to help it find your nodes.

4. Run the first node:
```sh
ros2 run vocal_command_pkg vocom_node
```

4. Open a new terminal, go inside the Docker container and run the second node:
```sh
docker exec -it epflxplore_vocal_command bash
. install/setup.bash
ros2 run vocal_command_pkg fake_cs_node
```

5. [Optionnal] How to create a ROS package (inside a running Docker container):
```sh
cd src
ros2 pkg create --build-type ament_python vocal_command_pkg
```

## TODO Ideas

- Fix Max audio duration print statement
- See how Ollama interact
- Refine prompt example with gradient, or calculus, or yards, and add goodbye
- Add good bye to sleep words
- Add 180_turn and 360_turn to direction
- Implementer move right, left, diagnally to the right (turn then move)
- add slowly or fast or normal as a JSON field for the execution speed
- Github copilot pour les dev
- Confirmer elca tool pour bien flag et debut de reponses aux tickets
- Reflechir aux pdm, rag   tiny rag, graph rag pour usage interne
- IA automatisation, pour les RH
- ajouter une key json execution speed (fast, slow, default)

installation: hardware should have the following folder for mic access and results output: volunme in dockecompose

Giovanni: 
- STOP PENDANT LA COMMANDE EXECUTION ( a tester) fait
- CHANGER EN SERVICE fait
- PUBLIER A LA FREQ X PENDANT X S fait 
- light non car s'allume en fonction de quel sous systeme ou non
- mode de deplacement du robot (lateral (en cours) ou normal)
- mode de nav
- drill: different etat (ex commencer a spin, spin+descente, spin+remontéee, apporter container pour les centres
drill down up)
- activer le drill seulement si la nav est pas allumée
- nerc rc dril -> new fsm (interface CS, drill node dans src)
- micro de logitec pas encore recu
- d'abord tester ou je parle a mon ordi et ca output juste les commandes au rover dans un topic de base 
- puis ou moi j'active la nav et je le dis a la CS 
- et enfin tout faire local

- Avant la rentrée de fevrier c'est chaud 

- 134G ca passe (nav c'est 15)
- l'ancienne va pas supporter , en attente des nouvelles qui sont censees etre arrivées de puis 2 semaines

Idées: 
  Version avec live Whisper, 
  continuer la SER (train?), 
  continuer la sentiment analysis (text)
  refaire une démo plus complète